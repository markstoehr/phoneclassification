% ---- ETD Document Class and Useful Packages ---- %
\documentclass{article}
\usepackage{subfigure,epsfig,amsfonts,bigints}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithmic,algorithm}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{graphicx}\usepackage{array}
\usepackage{dsfont}

%% Use these commands to set biographic information for the title page:
\title{Phone Classification Work}
\author{Mark Stoehr}
\date{\today}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{document}

\maketitle

\section{Introduction}

Here we examine the performance of linear classifiers on a phone-classification task.  The setup is that we have
extracted all the labeled phones from the TIMIT database and we consider generative and discriminative classification
algorithms, as well as multiple different feature sets.  The primary feature set we consider are eight coarse binary edge features
(corresponding to four directions and two polarities) which are computed
at every time-frequency location of a time-frequency representation of the data.  The classification algorithms we employ
are the linear-kernel SVM and a product of Bernoullis mixture model likelihood ratio test.

\section{Experiments}

We describe experiments on the various feature types with the two different classifiers, the problem we are considering is a large
scale multiclass problem.

\subsection{Edges}

We work with discrete prolate spheroidal sequence-window multitaper spectrograms and then compute binary edge features on those.

\section{SVM on Edges}

We reduce the multiclass classification problem to $\binom{C}{2}$ binary classification problems and then we choose the class with a vote
by the different classifiers.  Formally we have phone labels $0,1,\ldots,C-1$
and for each pair $(i,j)$ where $i<j$ we have a linear classifier
$w_{i,j}$.  Our label prediction function is
\begin{equation*}\begin{array}{rcl}
f(X ; {w_{i,j}}_{i<j}) &=&\underset{i}{\arg\max} \sum_{j<i} \mathds{1}_{w_{j,i}^{\top}x > 0} + \sum_{i<j} \mathds{1}_{w_{i,j}^{\top}x < 0}\\
&=& \underset{i}{\arg\max} \; (\max\{\mathbf{0},(\operatorname{sgn} B_iWx)\})^\top \mathbf{1}
\end{array}\end{equation*}
where $W$ is the matrix whose rows are the linear classifiers $w_{i,j}$ and $B_i$ is a diagonal matrix whose entry 
$$B_i(k,k) = \begin{cases} 1 & W(k,:)^{\top} = w_{j,i}\;\text{for some } j\text{ so }j<i\\
                       -1 & W(k,:)^\top = w_{i,j}\;\text{for some } j\text{ so }i<j\\
                    0 & \text{otherwise}   \end{cases}$$
and $\operatorname{sgn}$ is a pointwise signum operator so that $\max\{\mathbf{0},(\operatorname{sgn} B_iWx)\}$ is a binary vector
whose number of ones is the number of linear classifiers that classify in favor of $i$.
The other main multiclass strategy would be train a 1-vs-rest classifier.

The performance obtained with \texttt{scripts/run\_svm\_exp.sh} included in the code directory.

\begin{table}[h]
  \centering
  \begin{tabular}{| l |  r |}
    \hline
     Penalty & Error Rate \\ \hline\hline
     0.1 & \input{exp/all_phones_exp/svm_little_reg_leehon_errorrate.txt} \\
     \hline
     0.01 & \input{exp/all_phones_exp/svm_reg_plus_leehon_errorrate.txt} \\
     \hline
     0.001 & \input{exp/all_phones_exp/svm_reg_plus_plus_leehon_errorrate.txt} \\
     \hline
     0.0001 &  \input{exp/all_phones_exp/svm_reg_plus_plus_plus_leehon_errorrate.txt} \\
     \hline
  \end{tabular}
  \caption{SVM Error rates for different penalties over edge features}
  \label{tab:myfirsttable}
\end{table}
and we can also look at confusion matrices generated in \autoref{fig:confusion_matrix}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{exp/all_phones_exp/svm_all_pairs_reg_plus_leehon_confusion_rcm.png}
\caption{Confusion Matrix for Classifications}
\label{fig:confusion_matrix}
\end{figure}
and in that plot we capped the diagonal entries (correct classifications) at 200
in order to better visualize the mistakes since in this plot the
correct classifications would normally dominate the plot and the misakes would be very faint.  Each axis has all 39 phones used in the origin Lee and Hon experiments (based on the TIMIT labels).  Entry $i,j$ indicates the number of times phone $i$ was classified as phone $j$.  Thus, the matrix is not symmetric.

We then tested a standard multiclass SVM on the same data and
achieved an accuracy of only \input{exp/liblinear_all_phones/krammer_multiclass_out}.

\section{Likelihood Ratio Test on Edges}

The error rate achieved on the development data using the likelihood
ratio test was \input{exp/bernoulli_all_phones/bernoulli_dev_leehon_errorrate.txt} which is considerably worse than the performance by
the SVM shown in the previous section.  The confusion matrix is
visualized in \autoref{fig:bernoulli_confusion_matrix}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{exp/bernoulli_all_phones/bernoulli_dev_leehon_confusion_rcm.png}
\caption{Bernoulli Confusion Matrix for Classifications}
\label{fig:bernoulli_confusion_matrix}
\end{figure}


We also ran a number of experiments investigating whether the SVM could be incorporated to improve the likelihood ratio test results, 
but no combination surpassed the resuls obtained with the
SVM result.

\end{document}
